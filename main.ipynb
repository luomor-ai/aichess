{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用AlphaZero算法打造属于你自己的象棋AI!\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/70482fc2643343959250d272a3a252bcacadc365770b4bada4bd4f779e8b735e)\n",
    "\n",
    "## 一、从AlphaGo到AlphaZero，实现一套通用的游戏AI训练算法\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/96abb154ac8f45c2b154cf197b088edeb526dca97cb44a0e87819e1d10d22ec9)\n",
    "\n",
    "* 谷歌最早期的AlphaGo在2016年就已经可以击败围棋职业选手，但是当将AlphaGo应用到其他游戏上时却有很多困难，如：需要专家精心设计特征，需要人类玩家对弈的数据进行监督训练等。在迭代更新之后，一套全新的通用演算法AlphaZero出现了，理论上，只要是双人完全信息博弈的游戏都可以籍由此一套算法来进行AI训练，所以AlphaZero是一套通用演算法(不考虑算力成本)。\n",
    "\n",
    "* 对比于AlphaGo，AlphaZero主要对以下几点做了改进：\n",
    "\n",
    "> 1、AlphaGo用到了人类专家对弈的棋谱进行训练，AlphaZero则完全从零进行自对弈训练。避免了找专家数据的麻烦。\n",
    "\n",
    "> 2、AlphaGo用到了人类专家精心设计的特征，AlphaZero则只用到了棋盘的表示特征。让围棋小白也能训练出一个超强AI。\n",
    "\n",
    "> 3、AlphaZero棋力更强，是一个通用算法，理论上双人完全信息博弈类游戏皆可籍由此一个算法搞定。\n",
    "\n",
    "> 4、省去模型评估的部分，加速训练过程。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/ede8c74143f04aa6aadeb35695505d169641bcf3afaa4538aa98f6a481f4bfde)\n",
    "\n",
    "如上图所示，AlphaZero通过几十万次的自我对弈，最终要强于其他游戏软件和AlphaGo。\n",
    "\n",
    "## 二、本项目简介\n",
    "\n",
    "1、本项目的代码从零编写过程录制成了视频上传到b站，想认真分析代码的同学可以进行参考。另外，项目简介同样也录制了视频。\n",
    "\n",
    "2、本项目最好用多个进程进行训练，训练的方法如下：\n",
    "\n",
    "* 在终端运行\"**cd aichess**\"将路径定位到aichess里，然后运行\"**python collect.py**\"进行自我对弈数据的收集。可以开四个这样的终端进行更快速的数据收集。\n",
    "\n",
    "* 在终端运行\"**cd aichess**\"将路径定位到aichess里，然后运行\"**python train.py**\"进行模型的训练。这个只能开一个，因为我们的多进程是collect.py来实现的。\n",
    "\n",
    "3、训练差不多了之后，就可以进行快乐的人机对战了，可以在aistudio上运行\"play_with_ai.py\"进行print模式的对弈，也可以把项目copy到本地运行\"UIplay\"进行UI界面的一个对战。\n",
    "\n",
    "4、以下视频展示了训练了100次，蒙特卡洛搜索次数设置为4000次的一个人机对弈棋局，模型已经学会一步之内的将军防守，但不会预知两步之内的必杀将军。\n",
    "\n",
    "<div align=center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/3f018b0239ba42ff8324d4e9c417c96887fb952562c44c308024e55a1ee1949e\"></div>\n",
    "\n",
    "## 三、游戏棋盘表示\n",
    "\n",
    "接下来我们编写游戏棋盘表示，虽然AlphaZero不需要人类添加的特征，但是需要知道走子的规则。在这个游戏棋盘表示里面，我们会实现棋盘类用于棋盘表示，和游戏类用于游戏逻辑的控制，包括自我对弈，人机对弈等。这个代码非常长，主要是象棋本身的逻辑比较复杂，所以一开始上手本项目可以先看蒙特卡洛、神经网络、自我对弈训练部分。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"棋盘游戏控制\"\"\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "import time\n",
    "from config import CONFIG\n",
    "from collections import deque   # 这个队列用来判断长将或长捉\n",
    "import random\n",
    "\n",
    "\n",
    "# 列表来表示棋盘，红方在上，黑方在下。使用时需要使用深拷贝\n",
    "state_list_init = [['红车', '红马', '红象', '红士', '红帅', '红士', '红象', '红马', '红车'],\n",
    "                   ['一一', '一一', '一一', '一一', '一一', '一一', '一一', '一一', '一一'],\n",
    "                   ['一一', '红炮', '一一', '一一', '一一', '一一', '一一', '红炮', '一一'],\n",
    "                   ['红兵', '一一', '红兵', '一一', '红兵', '一一', '红兵', '一一', '红兵'],\n",
    "                   ['一一', '一一', '一一', '一一', '一一', '一一', '一一', '一一', '一一'],\n",
    "                   ['一一', '一一', '一一', '一一', '一一', '一一', '一一', '一一', '一一'],\n",
    "                   ['黑兵', '一一', '黑兵', '一一', '黑兵', '一一', '黑兵', '一一', '黑兵'],\n",
    "                   ['一一', '黑炮', '一一', '一一', '一一', '一一', '一一', '黑炮', '一一'],\n",
    "                   ['一一', '一一', '一一', '一一', '一一', '一一', '一一', '一一', '一一'],\n",
    "                   ['黑车', '黑马', '黑象', '黑士', '黑帅', '黑士', '黑象', '黑马', '黑车']]\n",
    "\n",
    "\n",
    "# deque来存储棋盘状态，长度为4\n",
    "state_deque_init = deque(maxlen=4)\n",
    "for _ in range(4):\n",
    "    state_deque_init.append(copy.deepcopy(state_list_init))\n",
    "\n",
    "\n",
    "# 构建一个字典：字符串到数组的映射，函数：数组到字符串的映射\n",
    "string2array = dict(红车=np.array([1, 0, 0, 0, 0, 0, 0]), 红马=np.array([0, 1, 0, 0, 0, 0, 0]),\n",
    "                    红象=np.array([0, 0, 1, 0, 0, 0, 0]), 红士=np.array([0, 0, 0, 1, 0, 0, 0]),\n",
    "                    红帅=np.array([0, 0, 0, 0, 1, 0, 0]), 红炮=np.array([0, 0, 0, 0, 0, 1, 0]),\n",
    "                    红兵=np.array([0, 0, 0, 0, 0, 0, 1]), 黑车=np.array([-1, 0, 0, 0, 0, 0, 0]),\n",
    "                    黑马=np.array([0, -1, 0, 0, 0, 0, 0]), 黑象=np.array([0, 0, -1, 0, 0, 0, 0]),\n",
    "                    黑士=np.array([0, 0, 0, -1, 0, 0, 0]), 黑帅=np.array([0, 0, 0, 0, -1, 0, 0]),\n",
    "                    黑炮=np.array([0, 0, 0, 0, 0, -1, 0]), 黑兵=np.array([0, 0, 0, 0, 0, 0, -1]),\n",
    "                    一一=np.array([0, 0, 0, 0, 0, 0, 0]))\n",
    "\n",
    "\n",
    "def array2string(array):\n",
    "    return list(filter(lambda string: (string2array[string] == array).all(), string2array))[0]\n",
    "\n",
    "\n",
    "# 改变棋盘状态\n",
    "def change_state(state_list, move):\n",
    "    \"\"\"move : 字符串'0010'\"\"\"\n",
    "    copy_list = copy.deepcopy(state_list)\n",
    "    y, x, toy, tox = int(move[0]), int(move[1]), int(move[2]), int(move[3])\n",
    "    copy_list[toy][tox] = copy_list[y][x]\n",
    "    copy_list[y][x] = '一一'\n",
    "    return copy_list\n",
    "\n",
    "\n",
    "# 打印盘面，可视化用到\n",
    "def print_board(_state_array):\n",
    "    # _state_array: [10, 9, 7], HWC\n",
    "    board_line = []\n",
    "    for i in range(10):\n",
    "        for j in range(9):\n",
    "            board_line.append(array2string(_state_array[i][j]))\n",
    "        print(board_line)\n",
    "        board_line.clear()\n",
    "\n",
    "\n",
    "# 列表棋盘状态到数组棋盘状态\n",
    "def state_list2state_array(state_list):\n",
    "    _state_array = np.zeros([10, 9, 7])\n",
    "    for i in range(10):\n",
    "        for j in range(9):\n",
    "            _state_array[i][j] = string2array[state_list[i][j]]\n",
    "    return _state_array\n",
    "\n",
    "\n",
    "# 拿到所有合法走子的集合，2086长度，也就是神经网络预测的走子概率向量的长度\n",
    "# 第一个字典：move_id到move_action\n",
    "# 第二个字典：move_action到move_id\n",
    "# 例如：move_id:0 --> move_action:'0010'\n",
    "def get_all_legal_moves():\n",
    "    _move_id2move_action = {}\n",
    "    _move_action2move_id = {}\n",
    "    row = ['0', '1', '2', '3', '4', '5', '6', '7', '8']\n",
    "    column = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "    # 士的全部走法\n",
    "    advisor_labels = ['0314', '1403', '0514', '1405', '2314', '1423', '2514', '1425',\n",
    "                      '9384', '8493', '9584', '8495', '7384', '8473', '7584', '8475']\n",
    "    # 象的全部走法\n",
    "    bishop_labels = ['2002', '0220', '2042', '4220', '0224', '2402', '4224', '2442',\n",
    "                     '2406', '0624', '2446', '4624', '0628', '2806', '4628', '2846',\n",
    "                     '7052', '5270', '7092', '9270', '5274', '7452', '9274', '7492',\n",
    "                     '7456', '5674', '7496', '9674', '5678', '7856', '9678', '7896']\n",
    "    idx = 0\n",
    "    for l1 in range(10):\n",
    "        for n1 in range(9):\n",
    "            destinations = [(t, n1) for t in range(10)] + \\\n",
    "                           [(l1, t) for t in range(9)] + \\\n",
    "                           [(l1 + a, n1 + b) for (a, b) in\n",
    "                            [(-2, -1), (-1, -2), (-2, 1), (1, -2), (2, -1), (-1, 2), (2, 1), (1, 2)]]  # 马走日\n",
    "            for (l2, n2) in destinations:\n",
    "                if (l1, n1) != (l2, n2) and l2 in range(10) and n2 in range(9):\n",
    "                    action = column[l1] + row[n1] + column[l2] + row[n2]\n",
    "                    _move_id2move_action[idx] = action\n",
    "                    _move_action2move_id[action] = idx\n",
    "                    idx += 1\n",
    "\n",
    "    for action in advisor_labels:\n",
    "        _move_id2move_action[idx] = action\n",
    "        _move_action2move_id[action] = idx\n",
    "        idx += 1\n",
    "\n",
    "    for action in bishop_labels:\n",
    "        _move_id2move_action[idx] = action\n",
    "        _move_action2move_id[action] = idx\n",
    "        idx += 1\n",
    "\n",
    "    return _move_id2move_action, _move_action2move_id\n",
    "\n",
    "\n",
    "move_id2move_action, move_action2move_id = get_all_legal_moves()\n",
    "\n",
    "\n",
    "# 走子翻转的函数，用来扩充我们的数据\n",
    "def flip_map(string):\n",
    "    new_str = ''\n",
    "    for index in range(4):\n",
    "        if index == 0 or index == 2:\n",
    "            new_str += (str(string[index]))\n",
    "        else:\n",
    "            new_str += (str(8 - int(string[index])))\n",
    "    return new_str\n",
    "\n",
    "\n",
    "# 边界检查\n",
    "def check_bounds(toY, toX):\n",
    "    if toY in [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] and toX in [0, 1, 2, 3, 4, 5, 6, 7, 8]:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# 不能走到自己的棋子位置\n",
    "def check_obstruct(piece, current_player_color):\n",
    "    # 当走到的位置存在棋子的时候，进行一次判断\n",
    "    if piece != '一一':\n",
    "        if current_player_color == '红':\n",
    "            if '黑' in piece:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "        elif current_player_color == '黑':\n",
    "            if '红' in piece:\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "    else:\n",
    "        return True\n",
    "\n",
    "\n",
    "# 得到当前盘面合法走子集合\n",
    "# 输入状态队列不能小于10，current_player_color:当前玩家控制的棋子颜色\n",
    "# 用来存放合法走子的列表，例如[0, 1, 2, 1089, 2085]\n",
    "def get_legal_moves(state_deque, current_player_color):\n",
    "    \"\"\"\n",
    "    ====\n",
    "      将\n",
    "    车\n",
    "    ====\n",
    "    ====\n",
    "      将\n",
    "      车\n",
    "    ====\n",
    "    ====\n",
    "    将\n",
    "      车\n",
    "    ====\n",
    "    ====\n",
    "    将\n",
    "    车\n",
    "    ====\n",
    "    ====\n",
    "      将\n",
    "    车\n",
    "    ====\n",
    "    这个时候，车就不能再往右走抓帅\n",
    "    接下来不能走的动作是(1011)，因为将会盘面与state_deque[-4]重复\n",
    "    \"\"\"\n",
    "\n",
    "    state_list = state_deque[-1]\n",
    "    old_state_list = state_deque[-4]\n",
    "\n",
    "    moves = []  # 用来存放所有合法的走子方法\n",
    "    face_to_face = False  # 将军面对面\n",
    "\n",
    "    # 记录将军的位置信息\n",
    "    k_x = None\n",
    "    k_y = None\n",
    "    K_x = None\n",
    "    K_y = None\n",
    "\n",
    "    # state_list是以列表形式表示的, len(state_list) == 10, len(state_list[0]) == 9\n",
    "    # 遍历移动初始位置\n",
    "    for y in range(10):\n",
    "        for x in range(9):\n",
    "            # 只有是棋子才可以移动\n",
    "            if state_list[y][x] == '一一':\n",
    "                pass\n",
    "            else:\n",
    "                if state_list[y][x] == '黑车' and current_player_color == '黑':  # 黑车的合法走子\n",
    "                    toY = y\n",
    "                    for toX in range(x - 1, -1, -1):\n",
    "                        # 前面是先前位置，后面是移动后的位置\n",
    "                        # 这里通过中断for循环实现了车的走子，车不能越过子\n",
    "                        m = str(y) + str(x) + str(toY) + str(toX)\n",
    "                        if state_list[toY][toX] != '一一':\n",
    "                            if '红' in state_list[toY][toX]:\n",
    "                                if change_state(state_list, m) != old_state_list:\n",
    "                                    moves.append(m)\n",
    "                            break\n",
    "                        if change_state(state_list, m) != old_state_list:\n",
    "                            moves.append(m)\n",
    "                    for toX in range(x + 1, 9):\n",
    "                        m = str(y) + str(x) + str(toY) + str(toX)\n",
    "                        if state_list[toY][toX] != '一一':\n",
    "                            if '红' in state_list[toY][toX]:\n",
    "                                if change_state(state_list, m) != old_state_list:\n",
    "                                    moves.append(m)\n",
    "                            break\n",
    "                        if change_state(state_list, m) != old_state_list:\n",
    "                            moves.append(m)\n",
    "\n",
    "                    toX = x\n",
    "                    for toY in range(y - 1, -1, -1):\n",
    "                        m = str(y) + str(x) + str(toY) + str(toX)\n",
    "                        if state_list[toY][toX] != '一一':\n",
    "                            if '红' in state_list[toY][toX]:\n",
    "                                if change_state(state_list, m) != old_state_list:\n",
    "                                    moves.append(m)\n",
    "                            break\n",
    "                        if change_state(state_list, m) != old_state_list:\n",
    "                            moves.append(m)\n",
    "                    for toY in range(y + 1, 10):\n",
    "                        m = str(y) + str(x) + str(toY) + str(toX)\n",
    "                        if state_list[toY][toX] != '一一':\n",
    "                            if '红' in state_list[toY][toX]:\n",
    "                                if change_state(state_list, m) != old_state_list:\n",
    "                                    moves.append(m)\n",
    "                            break\n",
    "                        if change_state(state_list, m) != old_state_list:\n",
    "                            moves.append(m)\n",
    "\n",
    "                elif state_list[y][x] == '红车' and current_player_color == '红':  # 红车的合法走子\n",
    "                    toY = y\n",
    "                    for toX in range(x - 1, -1, -1):\n",
    "                        # 前面是先前位置，后面是移动后的位置\n",
    "                        # 这里通过中断for循环实现了，车不能越过子\n",
    "                        m = str(y) + str(x) + str(toY) + str(toX)\n",
    "                        if state_list[toY][toX] != '一一':\n",
    "                            if '黑' in state_list[toY][toX]:\n",
    "                                if change_state(state_list, m) != old_state_list:\n",
    "                                    moves.append(m)\n",
    "                            break\n",
    "                        if change_state(state_list, m) != old_state_list:\n",
    "                            moves.append(m)\n",
    "                    for toX in range(x + 1, 9):\n",
    "                        m = str(y) + str(x) + str(toY) + str(toX)\n",
    "                        if state_list[toY][toX] != '一一':\n",
    "                            if '黑' in state_list[toY][toX]:\n",
    "                                if change_state(state_list, m) != old_state_list:\n",
    "                                    moves.append(m)\n",
    "                            break\n",
    "                        if change_state(state_list, m) != old_state_list:\n",
    "                            moves.append(m)\n",
    "\n",
    "                    toX = x\n",
    "                    for toY in range(y - 1, -1, -1):\n",
    "                        m = str(y) + str(x) + str(toY) + str(toX)\n",
    "                        if state_list[toY][toX] != '一一':\n",
    "                            if '黑' in state_list[toY][toX]:\n",
    "                                if change_state(state_list, m) != old_state_list:\n",
    "                                    moves.append(m)\n",
    "                            break\n",
    "                        if change_state(state_list, m) != old_state_list:\n",
    "                            moves.append(m)\n",
    "                    for toY in range(y + 1, 10):\n",
    "                        m = str(y) + str(x) + str(toY) + str(toX)\n",
    "                        if state_list[toY][toX] != '一一':\n",
    "                            if '黑' in state_list[toY][toX]:\n",
    "                                if change_state(state_list, m) != old_state_list:\n",
    "                                    moves.append(m)\n",
    "                            break\n",
    "                        if change_state(state_list, m) != old_state_list:\n",
    "                            moves.append(m)\n",
    "\n",
    "                # 黑马的合理走法\n",
    "                elif state_list[y][x] == '黑马' and current_player_color == '黑':\n",
    "                    for i in range(-1, 3, 2):\n",
    "                        for j in range(-1, 3, 2):\n",
    "                            toY = y + 2 * i\n",
    "                            toX = x + 1 * j\n",
    "                            if check_bounds(toY, toX) \\\n",
    "                                    and check_obstruct(state_list[toY][toX], current_player_color='黑') \\\n",
    "                                    and state_list[toY - i][x] == '一一':\n",
    "                                m = str(y) + str(x) + str(toY) + str(toX)\n",
    "                                if change_state(state_list, m) != old_state_list:\n",
    "                                    moves.append(m)\n",
    "                            toY = y + 1 * i\n",
    "                            toX = x + 2 * j\n",
    "                            if check_bounds(toY, toX) \\\n",
    "                                    and check_obstruct(state_list[toY][toX], current_player_color='黑') \\\n",
    "                                    and state_list[y][toX - j] == '一一':\n",
    "                                m = str(y) + str(x) + str(toY) + str(toX)\n",
    "                                if change_state(state_list, m) != old_state_list:\n",
    "                                    moves.append(m)\n",
    "\n",
    "                # 红马的合理走法\n",
    "                elif state_list[y][x] == '红马' and current_player_color == '红':\n",
    "                    for i in range(-1, 3, 2):\n",
    "                        for j in range(-1, 3, 2):\n",
    "                            toY = y + 2 * i\n",
    "                            toX = x + 1 * j\n",
    "                            if check_bounds(toY, toX) \\\n",
    "                                    and check_obstruct(state_list[toY][toX], current_player_color='红') \\\n",
    "                                    and state_list[toY - i][x] == '一一':\n",
    "                                m = str(y) + str(x) + str(toY) + str(toX)\n",
    "                                if change_state(state_list, m) != old_state_list:\n",
    "                                    moves.append(m)\n",
    "                            toY = y + 1 * i\n",
    "                            toX = x + 2 * j\n",
    "                            if check_bounds(toY, toX) \\\n",
    "                                    and check_obstruct(state_list[toY][toX], current_player_color='红') \\\n",
    "                                    and state_list[y][toX - j] == '一一':\n",
    "                                m = str(y) + str(x) + str(toY) + str(toX)\n",
    "                                if change_state(state_list, m) != old_state_list:\n",
    "                                    moves.append(m)\n",
    "\n",
    "                # 黑象的合理走法\n",
    "                elif state_list[y][x] == '黑象' and current_player_color == '黑':\n",
    "                    for i in range(-2, 3, 4):\n",
    "                        toY = y + i\n",
    "                        toX = x + i\n",
    "                        if check_bounds(toY, toX) \\\n",
    "                                and check_obstruct(state_list[toY][toX], current_player_color='黑') \\\n",
    "                                and toY >= 5 and state_list[y + i // 2][x + i // 2] == '一一':\n",
    "                            m = str(y) + str(x) + str(toY) + str(toX)\n",
    "                            if change_state(state_list, m) != old_state_list:\n",
    "                                moves.append(m)\n",
    "                        toY = y + i\n",
    "                        toX = x - i\n",
    "                        if check_bounds(toY, toX) \\\n",
    "                                and check_obstruct(state_list[toY][toX], current_player_color='黑') \\\n",
    "                                and toY >= 5 and state_list[y + i // 2][x - i // 2] == '一一':\n",
    "                            m = str(y) + str(x) + str(toY) + str(toX)\n",
    "                            if change_state(state_list, m) != old_state_list:\n",
    "                                moves.append(m)\n",
    "\n",
    "                # 红象的合理走法\n",
    "                elif state_list[y][x] == '红象' and current_player_color == '红':\n",
    "                    for i in range(-2, 3, 4):\n",
    "                        toY = y + i\n",
    "                        toX = x + i\n",
    "                        if check_bounds(toY, toX) \\\n",
    "                                and check_obstruct(state_list[toY][toX], current_player_color='红') \\\n",
    "                                and toY <= 4 and state_list[y + i // 2][x + i // 2] == '一一':\n",
    "                            m = str(y) + str(x) + str(toY) + str(toX)\n",
    "                            if change_state(state_list, m) != old_state_list:\n",
    "                                moves.append(m)\n",
    "                        toY = y + i\n",
    "                        toX = x - i\n",
    "                        if check_bounds(toY, toX) \\\n",
    "                                and check_obstruct(state_list[toY][toX], current_player_color='红') \\\n",
    "                                and toY <= 4 and state_list[y + i // 2][x - i // 2] == '一一':\n",
    "                            m = str(y) + str(x) + str(toY) + str(toX)\n",
    "                            if change_state(state_list, m) != old_state_list:\n",
    "                                moves.append(m)\n",
    "\n",
    "                # 黑士的合理走法\n",
    "                elif state_list[y][x] == '黑士' and current_player_color == '黑':\n",
    "                    for i in range(-1, 3, 2):\n",
    "                        toY = y + i\n",
    "                        toX = x + i\n",
    "                        if check_bounds(toY, toX) and check_obstruct(state_list[toY][toX], current_player_color='黑') \\\n",
    "                                and toY >= 7 and 3 <= toX <= 5:\n",
    "                            m = str(y) + str(x) + str(toY) + str(toX)\n",
    "                            if change_state(state_list, m) != old_state_list:\n",
    "                                moves.append(m)\n",
    "                        toY = y + i\n",
    "                        toX = x - i\n",
    "                        if check_bounds(toY, toX) and check_obstruct(state_list[toY][toX], current_player_color='黑') \\\n",
    "                                and toY >= 7 and 3 <= toX <= 5:\n",
    "                            m = str(y) + str(x) + str(toY) + str(toX)\n",
    "                            if change_state(state_list, m) != old_state_list:\n",
    "                                moves.append(m)\n",
    "\n",
    "                # 红士的合理走法\n",
    "                elif state_list[y][x] == '红士' and current_player_color == '红':\n",
    "                    for i in range(-1, 3, 2):\n",
    "                        toY = y + i\n",
    "                        toX = x + i\n",
    "                        if check_bounds(toY, toX) and check_obstruct(state_list[toY][toX], current_player_color='红') \\\n",
    "                                and toY <= 2 and 3 <= toX <= 5:\n",
    "                            m = str(y) + str(x) + str(toY) + str(toX)\n",
    "                            if change_state(state_list, m) != old_state_list:\n",
    "                                moves.append(m)\n",
    "                        toY = y + i\n",
    "                        toX = x - i\n",
    "                        if check_bounds(toY, toX) and check_obstruct(state_list[toY][toX], current_player_color='红') \\\n",
    "                                and toY <= 2 and 3 <= toX <= 5:\n",
    "                            m = str(y) + str(x) + str(toY) + str(toX)\n",
    "                            if change_state(state_list, m) != old_state_list:\n",
    "                                moves.append(m)\n",
    "\n",
    "                # 黑帅的合理走法\n",
    "                elif state_list[y][x] == '黑帅':\n",
    "                    k_x = x\n",
    "                    k_y = y\n",
    "                    if current_player_color == '黑':\n",
    "                        for i in range(2):\n",
    "                            for sign in range(-1, 2, 2):\n",
    "                                j = 1 - i\n",
    "                                toY = y + i * sign\n",
    "                                toX = x + j * sign\n",
    "\n",
    "                                if check_bounds(toY, toX) and check_obstruct(\n",
    "                                        state_list[toY][toX], current_player_color='黑') and toY >= 7 and 3 <= toX <= 5:\n",
    "                                    m = str(y) + str(x) + str(toY) + str(toX)\n",
    "                                    if change_state(state_list, m) != old_state_list:\n",
    "                                        moves.append(m)\n",
    "\n",
    "                # 红帅的合理走法\n",
    "                elif state_list[y][x] == '红帅':\n",
    "                    K_x = x\n",
    "                    K_y = y\n",
    "                    if current_player_color == '红':\n",
    "                        for i in range(2):\n",
    "                            for sign in range(-1, 2, 2):\n",
    "                                j = 1 - i\n",
    "                                toY = y + i * sign\n",
    "                                toX = x + j * sign\n",
    "\n",
    "                                if check_bounds(toY, toX) and check_obstruct(\n",
    "                                        state_list[toY][toX], current_player_color='红') and toY <= 2 and 3 <= toX <= 5:\n",
    "                                    m = str(y) + str(x) + str(toY) + str(toX)\n",
    "                                    if change_state(state_list, m) != old_state_list:\n",
    "                                        moves.append(m)\n",
    "\n",
    "                # 黑炮的合理走法\n",
    "                elif state_list[y][x] == '黑炮' and current_player_color == '黑':\n",
    "                    toY = y\n",
    "                    hits = False\n",
    "                    for toX in range(x - 1, -1, -1):\n",
    "                        m = str(y) + str(x) + str(toY) + str(toX)\n",
    "                        if hits is False:\n",
    "                            if state_list[toY][toX] != '一一':\n",
    "                                hits = True\n",
    "                            else:\n",
    "                                if change_state(state_list, m) != old_state_list:\n",
    "                                    moves.append(m)\n",
    "                        else:\n",
    "                            if state_list[toY][toX] != '一一':\n",
    "                                if '红' in state_list[toY][toX]:\n",
    "                                    if change_state(state_list, m) != old_state_list:\n",
    "                                        moves.append(m)\n",
    "                                break\n",
    "                    hits = False\n",
    "                    for toX in range(x + 1, 9):\n",
    "                        m = str(y) + str(x) + str(toY) + str(toX)\n",
    "                        if hits is False:\n",
    "                            if state_list[toY][toX] != '一一':\n",
    "                                hits = True\n",
    "                            else:\n",
    "                                if change_state(state_list, m) != old_state_list:\n",
    "                                    moves.append(m)\n",
    "                        else:\n",
    "                            if state_list[toY][toX] != '一一':\n",
    "                                if '红' in state_list[toY][toX]:\n",
    "                                    if change_state(state_list, m) != old_state_list:\n",
    "                                        moves.append(m)\n",
    "                                break\n",
    "\n",
    "                    toX = x\n",
    "                    hits = False\n",
    "                    for toY in range(y - 1, -1, -1):\n",
    "                        m = str(y) + str(x) + str(toY) + str(toX)\n",
    "                        if hits is False:\n",
    "                            if state_list[toY][toX] != '一一':\n",
    "                                hits = True\n",
    "                            else:\n",
    "                                if change_state(state_list, m) != old_state_list:\n",
    "                                    moves.append(m)\n",
    "                        else:\n",
    "                            if state_list[toY][toX] != '一一':\n",
    "                                if '红' in state_list[toY][toX]:\n",
    "                                    if change_state(state_list, m) != old_state_list:\n",
    "                                        moves.append(m)\n",
    "                                break\n",
    "                    hits = False\n",
    "                    for toY in range(y + 1, 10):\n",
    "                        m = str(y) + str(x) + str(toY) + str(toX)\n",
    "                        if hits is False:\n",
    "                            if state_list[toY][toX] != '一一':\n",
    "                                hits = True\n",
    "                            else:\n",
    "                                if change_state(state_list, m) != old_state_list:\n",
    "                                    moves.append(m)\n",
    "                        else:\n",
    "                            if state_list[toY][toX] != '一一':\n",
    "                                if '红' in state_list[toY][toX]:\n",
    "                                    if change_state(state_list, m) != old_state_list:\n",
    "                                        moves.append(m)\n",
    "                                break\n",
    "\n",
    "                # 红炮的合理走法\n",
    "                elif state_list[y][x] == '红炮' and current_player_color == '红':\n",
    "                    toY = y\n",
    "                    hits = False\n",
    "                    for toX in range(x - 1, -1, -1):\n",
    "                        m = str(y) + str(x) + str(toY) + str(toX)\n",
    "                        if hits is False:\n",
    "                            if state_list[toY][toX] != '一一':\n",
    "                                hits = True\n",
    "                            else:\n",
    "                                if change_state(state_list, m) != old_state_list:\n",
    "                                    moves.append(m)\n",
    "                        else:\n",
    "                            if state_list[toY][toX] != '一一':\n",
    "                                if '黑' in state_list[toY][toX]:\n",
    "                                    if change_state(state_list, m) != old_state_list:\n",
    "                                        moves.append(m)\n",
    "                                break\n",
    "                    hits = False\n",
    "                    for toX in range(x + 1, 9):\n",
    "                        m = str(y) + str(x) + str(toY) + str(toX)\n",
    "                        if hits is False:\n",
    "                            if state_list[toY][toX] != '一一':\n",
    "                                hits = True\n",
    "                            else:\n",
    "                                if change_state(state_list, m) != old_state_list:\n",
    "                                    moves.append(m)\n",
    "                        else:\n",
    "                            if state_list[toY][toX] != '一一':\n",
    "                                if '黑' in state_list[toY][toX]:\n",
    "                                    if change_state(state_list, m) != old_state_list:\n",
    "                                        moves.append(m)\n",
    "                                break\n",
    "\n",
    "                    toX = x\n",
    "                    hits = False\n",
    "                    for toY in range(y - 1, -1, -1):\n",
    "                        m = str(y) + str(x) + str(toY) + str(toX)\n",
    "                        if hits is False:\n",
    "                            if state_list[toY][toX] != '一一':\n",
    "                                hits = True\n",
    "                            else:\n",
    "                                if change_state(state_list, m) != old_state_list:\n",
    "                                    moves.append(m)\n",
    "                        else:\n",
    "                            if state_list[toY][toX] != '一一':\n",
    "                                if '黑' in state_list[toY][toX]:\n",
    "                                    if change_state(state_list, m) != old_state_list:\n",
    "                                        moves.append(m)\n",
    "                                break\n",
    "                    hits = False\n",
    "                    for toY in range(y + 1, 10):\n",
    "                        m = str(y) + str(x) + str(toY) + str(toX)\n",
    "                        if hits is False:\n",
    "                            if state_list[toY][toX] != '一一':\n",
    "                                hits = True\n",
    "                            else:\n",
    "                                if change_state(state_list, m) != old_state_list:\n",
    "                                    moves.append(m)\n",
    "                        else:\n",
    "                            if state_list[toY][toX] != '一一':\n",
    "                                if '黑' in state_list[toY][toX]:\n",
    "                                    if change_state(state_list, m) != old_state_list:\n",
    "                                        moves.append(m)\n",
    "                                break\n",
    "\n",
    "                # 黑兵的合法走子\n",
    "                elif state_list[y][x] == '黑兵' and current_player_color == '黑':\n",
    "                    toY = y - 1\n",
    "                    toX = x\n",
    "                    if check_bounds(toY, toX) and check_obstruct(state_list[toY][toX], current_player_color='黑'):\n",
    "                        m = str(y) + str(x) + str(toY) + str(toX)\n",
    "                        if change_state(state_list, m) != old_state_list:\n",
    "                            moves.append(m)\n",
    "                    # 小兵过河\n",
    "                    if y < 5:\n",
    "                        toY = y\n",
    "                        toX = x + 1\n",
    "                        if check_bounds(toY, toX) and check_obstruct(state_list[toY][toX], current_player_color='黑'):\n",
    "                            m = str(y) + str(x) + str(toY) + str(toX)\n",
    "                            if change_state(state_list, m) != old_state_list:\n",
    "                                moves.append(m)\n",
    "                        toX = x - 1\n",
    "                        if check_bounds(toY, toX) and check_obstruct(state_list[toY][toX], current_player_color='黑'):\n",
    "                            m = str(y) + str(x) + str(toY) + str(toX)\n",
    "                            if change_state(state_list, m) != old_state_list:\n",
    "                                moves.append(m)\n",
    "\n",
    "                # 红兵的合法走子\n",
    "                elif state_list[y][x] == '红兵' and current_player_color == '红':\n",
    "                    toY = y + 1\n",
    "                    toX = x\n",
    "                    if check_bounds(toY, toX) and check_obstruct(state_list[toY][toX], current_player_color='红'):\n",
    "                        m = str(y) + str(x) + str(toY) + str(toX)\n",
    "                        if change_state(state_list, m) != old_state_list:\n",
    "                            moves.append(m)\n",
    "                    # 小兵过河\n",
    "                    if y > 4:\n",
    "                        toY = y\n",
    "                        toX = x + 1\n",
    "                        if check_bounds(toY, toX) and check_obstruct(state_list[toY][toX], current_player_color='红'):\n",
    "                            m = str(y) + str(x) + str(toY) + str(toX)\n",
    "                            if change_state(state_list, m) != old_state_list:\n",
    "                                moves.append(m)\n",
    "                        toX = x - 1\n",
    "                        if check_bounds(toY, toX) and check_obstruct(state_list[toY][toX], current_player_color='红'):\n",
    "                            m = str(y) + str(x) + str(toY) + str(toX)\n",
    "                            if change_state(state_list, m) != old_state_list:\n",
    "                                moves.append(m)\n",
    "\n",
    "    if K_x is not None and k_x is not None and K_x == k_x:\n",
    "        face_to_face = True\n",
    "        for i in range(K_y + 1, k_y, 1):\n",
    "            if state_list[i][K_x] != '一一':\n",
    "                face_to_face = False\n",
    "\n",
    "    if face_to_face is True:\n",
    "        if current_player_color == '黑':\n",
    "            m = str(k_y) + str(k_x) + str(K_y) + str(K_x)\n",
    "            if change_state(state_list, m) != old_state_list:\n",
    "                moves.append(m)\n",
    "        else:\n",
    "            m = str(K_y) + str(K_x) + str(k_y) + str(k_x)\n",
    "            if change_state(state_list, m) != old_state_list:\n",
    "                moves.append(m)\n",
    "\n",
    "    moves_id = []\n",
    "    for move in moves:\n",
    "        moves_id.append(move_action2move_id[move])\n",
    "    return moves_id\n",
    "\n",
    "\n",
    "# 棋盘逻辑控制\n",
    "class Board(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.state_list = copy.deepcopy(state_list_init)\n",
    "        self.game_start = False\n",
    "        self.winner = None\n",
    "        self.state_deque = copy.deepcopy(state_deque_init)\n",
    "\n",
    "    # 初始化棋盘的方法\n",
    "    def init_board(self, start_player=1):   # 传入先手玩家的id\n",
    "        # 增加一个颜色到id的映射字典，id到颜色的映射字典\n",
    "        # 永远是红方先移动\n",
    "        if start_player == 1:\n",
    "            self.id2color = {1: '红', 2: '黑'}\n",
    "            self.color2id = {'红': 1, '黑': 2}\n",
    "        elif start_player == 2:\n",
    "            self.id2color = {2: '红', 1: '黑'}\n",
    "            self.color2id = {'红': 2, '黑': 1}\n",
    "        # 当前手玩家，也就是先手玩家\n",
    "        self.current_player_color = self.id2color[start_player]     # 红\n",
    "        self.current_player_id = self.color2id['红']\n",
    "        # 初始化棋盘状态\n",
    "        self.state_list = copy.deepcopy(state_list_init)\n",
    "        self.state_deque = copy.deepcopy(state_deque_init)\n",
    "        # 初始化最后落子位置\n",
    "        self.last_move = -1\n",
    "        # 记录游戏中吃子的回合数\n",
    "        self.kill_action = 0\n",
    "        self.game_start = False\n",
    "        self.action_count = 0   # 游戏动作计数器\n",
    "        self.winner = None\n",
    "\n",
    "    @property\n",
    "    # 获的当前盘面的所有合法走子集合\n",
    "    def availables(self):\n",
    "        return get_legal_moves(self.state_deque, self.current_player_color)\n",
    "\n",
    "    # 从当前玩家的视角返回棋盘状态，current_state_array: [9, 10, 9]  CHW\n",
    "    def current_state(self):\n",
    "        _current_state = np.zeros([9, 10, 9])\n",
    "        # 使用9个平面来表示棋盘状态\n",
    "        # 0-6个平面表示棋子位置，1代表红方棋子，-1代表黑方棋子, 队列最后一个盘面\n",
    "        # 第7个平面表示对手player最近一步的落子位置，走子之前的位置为-1，走子之后的位置为1，其余全部是0\n",
    "        # 第8个平面表示的是当前player是不是先手player，如果是先手player则整个平面全部为1，否则全部为0\n",
    "        if self.game_start:\n",
    "            _current_state[:7] = state_list2state_array(self.state_deque[-1]).transpose([2, 0, 1])  # [7, 10, 9]\n",
    "            # 解构self.last_move\n",
    "            move = move_id2move_action[self.last_move]\n",
    "            start_position = int(move[0]), int(move[1])\n",
    "            end_position = int(move[2]), int(move[3])\n",
    "            _current_state[7][start_position[0]][start_position[1]] = -1\n",
    "            _current_state[7][end_position[0]][end_position[1]] = 1\n",
    "        # 指出当前是哪个玩家走子\n",
    "        if self.action_count % 2 == 0:\n",
    "            _current_state[8][:, :] = 1.0\n",
    "\n",
    "        return _current_state\n",
    "\n",
    "    # 根据move对棋盘状态做出改变\n",
    "    def do_move(self, move):\n",
    "        self.game_start = True  # 游戏开始\n",
    "        self.action_count += 1  # 移动次数加1\n",
    "        move_action = move_id2move_action[move]\n",
    "        start_y, start_x = int(move_action[0]), int(move_action[1])\n",
    "        end_y, end_x = int(move_action[2]), int(move_action[3])\n",
    "        state_list = copy.deepcopy(self.state_deque[-1])\n",
    "        # 判断是否吃子\n",
    "        if state_list[end_y][end_x] != '一一':\n",
    "            # 如果吃掉对方的帅，则返回当前的current_player胜利\n",
    "            self.kill_action = 0\n",
    "            if self.current_player_color == '黑' and state_list[end_y][end_x] == '红帅':\n",
    "                self.winner = self.color2id['黑']\n",
    "            elif self.current_player_color == '红' and state_list[end_y][end_x] == '黑帅':\n",
    "                self.winner = self.color2id['红']\n",
    "        else:\n",
    "            self.kill_action += 1\n",
    "        # 更改棋盘状态\n",
    "        state_list[end_y][end_x] = state_list[start_y][start_x]\n",
    "        state_list[start_y][start_x] = '一一'\n",
    "        self.current_player_color = '黑' if self.current_player_color == '红' else '红'  # 改变当前玩家\n",
    "        self.current_player_id = 1 if self.current_player_id == 2 else 2\n",
    "        # 记录最后一次移动的位置\n",
    "        self.last_move = move\n",
    "        self.state_deque.append(state_list)\n",
    "\n",
    "    # 是否产生赢家\n",
    "    def has_a_winner(self):\n",
    "        \"\"\"一共有三种状态，红方胜，黑方胜，平局\"\"\"\n",
    "        if self.winner is not None:\n",
    "            return True, self.winner\n",
    "        elif self.kill_action >= CONFIG['kill_action']:  # 平局\n",
    "            return False, -1\n",
    "        return False, -1\n",
    "\n",
    "    # 检查当前棋局是否结束\n",
    "    def game_end(self):\n",
    "        win, winner = self.has_a_winner()\n",
    "        if win:\n",
    "            return True, winner\n",
    "        elif self.kill_action >= CONFIG['kill_action']:  # 平局，没有赢家\n",
    "            return True, -1\n",
    "        return False, -1\n",
    "\n",
    "    def get_current_player_color(self):\n",
    "        return self.current_player_color\n",
    "\n",
    "    def get_current_player_id(self):\n",
    "        return self.current_player_id\n",
    "\n",
    "\n",
    "# 在Board类基础上定义Game类，该类用于启动并控制一整局对局的完整流程，并收集对局过程中的数据，以及进行棋盘的展示\n",
    "class Game(object):\n",
    "\n",
    "    def __init__(self, board):\n",
    "        self.board = board\n",
    "\n",
    "    # 可视化\n",
    "    def graphic(self, board, player1_color, player2_color):\n",
    "        print('player1 take: ', player1_color)\n",
    "        print('player2 take: ', player2_color)\n",
    "        print_board(state_list2state_array(board.state_deque[-1]))\n",
    "\n",
    "    # 用于人机对战，人人对战等\n",
    "    def start_play(self, player1, player2, start_player=1, is_shown=1):\n",
    "        if start_player not in (1, 2):\n",
    "            raise Exception('start_player should be either 1 (player1 first) '\n",
    "                            'or 2 (player2 first)')\n",
    "        self.board.init_board(start_player)  # 初始化棋盘\n",
    "        p1, p2 = 1, 2\n",
    "        player1.set_player_ind(1)\n",
    "        player2.set_player_ind(2)\n",
    "        players = {p1: player1, p2: player2}\n",
    "        if is_shown:\n",
    "            self.graphic(self.board, player1.player, player2.player)\n",
    "\n",
    "        while True:\n",
    "            current_player = self.board.get_current_player_id()  # 红子对应的玩家id\n",
    "            player_in_turn = players[current_player]  # 决定当前玩家的代理\n",
    "            move = player_in_turn.get_action(self.board)  # 当前玩家代理拿到动作\n",
    "            self.board.do_move(move)  # 棋盘做出改变\n",
    "            if is_shown:\n",
    "                self.graphic(self.board, player1.player, player2.player)\n",
    "            end, winner = self.board.game_end()\n",
    "            if end:\n",
    "                if winner != -1:\n",
    "                    print(\"Game end. Winner is\", players[winner])\n",
    "                else:\n",
    "                    print(\"Game end. Tie\")\n",
    "                return winner\n",
    "\n",
    "    # 使用蒙特卡洛树搜索开始自我对弈，存储游戏状态（状态，蒙特卡洛落子概率，胜负手）三元组用于神经网络训练\n",
    "    def start_self_play(self, player, is_shown=False, temp=1e-3):\n",
    "        self.board.init_board()     # 初始化棋盘, start_player=1\n",
    "        p1, p2 = 1, 2\n",
    "        states, mcts_probs, current_players = [], [], []\n",
    "        # 开始自我对弈\n",
    "        _count = 0\n",
    "        while True:\n",
    "            _count += 1\n",
    "            if _count % 20 == 0:\n",
    "                start_time = time.time()\n",
    "                move, move_probs = player.get_action(self.board,\n",
    "                                                     temp=temp,\n",
    "                                                     return_prob=1)\n",
    "                print('走一步要花: ', time.time() - start_time)\n",
    "            else:\n",
    "                move, move_probs = player.get_action(self.board,\n",
    "                                                     temp=temp,\n",
    "                                                     return_prob=1)\n",
    "            # 保存自我对弈的数据\n",
    "            states.append(self.board.current_state())\n",
    "            mcts_probs.append(move_probs)\n",
    "            current_players.append(self.board.current_player_id)\n",
    "            # 执行一步落子\n",
    "            self.board.do_move(move)\n",
    "            end, winner = self.board.game_end()\n",
    "            if end:\n",
    "                # 从每一个状态state对应的玩家的视角保存胜负信息\n",
    "                winner_z = np.zeros(len(current_players))\n",
    "                if winner != -1:\n",
    "                    winner_z[np.array(current_players) == winner] = 1.0\n",
    "                    winner_z[np.array(current_players) != winner] = -1.0\n",
    "                # 重置蒙特卡洛根节点\n",
    "                player.reset_player()\n",
    "                if is_shown:\n",
    "                    if winner != -1:\n",
    "                        print(\"Game end. Winner is:\", winner)\n",
    "                    else:\n",
    "                        print('Game end. Tie')\n",
    "\n",
    "                return winner, zip(states, mcts_probs, winner_z)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \"\"\"# 测试array2string\n",
    "    _array = np.array([0, 0, 0, 0, 0, 0, 0])\n",
    "    print(array2string(_array))\"\"\"\n",
    "\n",
    "    \"\"\"# 测试change_state\n",
    "    new_state = change_state(state_list_init, move='0010')\n",
    "    for row in range(10):\n",
    "        print(new_state[row])\"\"\"\n",
    "\n",
    "    \"\"\"# 测试print_board\n",
    "    _state_list = copy.deepcopy(state_list_init)\n",
    "    print_board(state_list2state_array(_state_list))\"\"\"\n",
    "\n",
    "    \"\"\"# 测试get_legal_moves\n",
    "    moves = get_legal_moves(state_deque_init, current_player_color='黑')\n",
    "    move_actions = []\n",
    "    for item in moves:\n",
    "        move_actions.append(move_id2move_action[item])\n",
    "    print(move_actions)\"\"\"\n",
    "\n",
    "    # 测试Board中的start_play\n",
    "    class Human1:\n",
    "        def get_action(self, board):\n",
    "            # print('当前是player1在操作')\n",
    "            # print(board.current_player_color)\n",
    "            # move = move_action2move_id[input('请输入')]\n",
    "            move = random.choice(board.availables)\n",
    "            return move\n",
    "\n",
    "        def set_player_ind(self, p):\n",
    "            self.player = p\n",
    "\n",
    "\n",
    "    class Human2:\n",
    "        def get_action(self, board):\n",
    "            # print('当前是player2在操作')\n",
    "            # print(board.current_player_color)\n",
    "            # move = move_action2move_id[input('请输入')]\n",
    "            move = random.choice(board.availables)\n",
    "            return move\n",
    "\n",
    "        def set_player_ind(self, p):\n",
    "            self.player = p\n",
    "\n",
    "    human1 = Human1()\n",
    "    human2 = Human2()\n",
    "    game = Game(board=Board())\n",
    "    for i in range(20):\n",
    "        game.start_play(human1, human2, start_player=2, is_shown=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四、神经网络预测\n",
    "\n",
    "* 这一部分，让我们来创建神经网络。网络输入是当前的棋盘状态，然后经过前向传播返回走子概率向量和盘面价值。损失函数包含三部分：1、l2正则化，在优化器中定义；2、mse_loss，价值评估的损失（蒙特卡洛树搜索得到的价值和神经网络预测之间的误差）；3、概率向量一致性损失（蒙特卡洛树搜索得到的概率向量和神经网络预测之间的误差）\n",
    "\n",
    "* 我们的神经网络将用于蒙特卡洛树搜索中，实现对树搜索过程中遇见的节点进行一个价值的评估，和赋予当前节点所有子节点的先验概率的功能。\n",
    "\n",
    "* 我们通过神经网络来对搜索的宽度和深度进行一个裁剪，同时我们使用蒙特卡洛树搜索得到的数据来对神经网络进行一个训练。神经网络和蒙特卡洛搜索树是一个相辅相成的关系。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/2a823c3c91b243428508697f5f788a55892a90126bde4a6e85d0dd2c5a7b0a0c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"策略价值网络\"\"\"\r\n",
    "\r\n",
    "\r\n",
    "import paddle\r\n",
    "import paddle.nn as nn\r\n",
    "import numpy as np\r\n",
    "import paddle.nn.functional as F\r\n",
    "\r\n",
    "\r\n",
    "# 搭建残差块\r\n",
    "class ResBlock(nn.Layer):\r\n",
    "\r\n",
    "    def __init__(self, num_filters=256):\r\n",
    "        super().__init__()\r\n",
    "        self.conv1 = nn.Conv2D(in_channels=num_filters, out_channels=num_filters, kernel_size=3, stride=1, padding=1)\r\n",
    "        self.conv1_bn = nn.BatchNorm2D(num_features=num_filters)\r\n",
    "        self.conv1_act = nn.ReLU()\r\n",
    "        self.conv2 = nn.Conv2D(in_channels=num_filters, out_channels=num_filters, kernel_size=3, stride=1, padding=1)\r\n",
    "        self.conv2_bn = nn.BatchNorm2D(num_features=num_filters)\r\n",
    "        self.conv2_act = nn.ReLU()\r\n",
    "\r\n",
    "    def forward(self, x):\r\n",
    "        y = self.conv1(x)\r\n",
    "        y = self.conv1_bn(y)\r\n",
    "        y = self.conv1_act(y)\r\n",
    "        y = self.conv2(y)\r\n",
    "        y = self.conv2_bn(y)\r\n",
    "        y = x + y\r\n",
    "        return self.conv2_act(y)\r\n",
    "\r\n",
    "\r\n",
    "# 搭建骨干网络，输入：N, 9, 10, 9 --> N, C, H, W\r\n",
    "class Net(nn.Layer):\r\n",
    "\r\n",
    "    def __init__(self, num_channels=256, num_res_blocks=7):\r\n",
    "        super().__init__()\r\n",
    "        # 初始化特征\r\n",
    "        self.conv_block = nn.Conv2D(in_channels=9, out_channels=num_channels, kernel_size=3, stride=1, padding=1)\r\n",
    "        self.conv_block_bn = nn.BatchNorm2D(num_features=256)\r\n",
    "        self.conv_block_act = nn.ReLU()\r\n",
    "        # 残差块抽取特征\r\n",
    "        self.res_blocks = nn.LayerList([ResBlock(num_filters=num_channels) for _ in range(num_res_blocks)])\r\n",
    "        # 策略头\r\n",
    "        self.policy_conv = nn.Conv2D(in_channels=num_channels, out_channels=16, kernel_size=1, stride=1)\r\n",
    "        self.policy_bn = nn.BatchNorm2D(16)\r\n",
    "        self.policy_act = nn.ReLU()\r\n",
    "        self.policy_fc = nn.Linear(16 * 9 * 10, 2086)\r\n",
    "        # 价值头\r\n",
    "        self.value_conv = nn.Conv2D(in_channels=num_channels, out_channels=8, kernel_size=1, stride=1)\r\n",
    "        self.value_bn = nn.BatchNorm2D(8)\r\n",
    "        self.value_act1 = nn.ReLU()\r\n",
    "        self.value_fc1 = nn.Linear(8 * 9 * 10, 256)\r\n",
    "        self.value_act2 = nn.ReLU()\r\n",
    "        self.value_fc2 = nn.Linear(256, 1)\r\n",
    "\r\n",
    "    # 定义前向传播\r\n",
    "    def forward(self, x):\r\n",
    "        # 公共头\r\n",
    "        x = self.conv_block(x)\r\n",
    "        x = self.conv_block_bn(x)\r\n",
    "        x = self.conv_block_act(x)\r\n",
    "        for layer in self.res_blocks:\r\n",
    "            x = layer(x)\r\n",
    "        # 策略头\r\n",
    "        policy = self.policy_conv(x)\r\n",
    "        policy = self.policy_bn(policy)\r\n",
    "        policy = self.policy_act(policy)\r\n",
    "        policy = paddle.reshape(policy, [-1, 16 * 10 * 9])\r\n",
    "        policy = self.policy_fc(policy)\r\n",
    "        policy = F.log_softmax(policy)\r\n",
    "        # 价值头\r\n",
    "        value = self.value_conv(x)\r\n",
    "        value = self.value_bn(value)\r\n",
    "        value = self.value_act1(value)\r\n",
    "        value = paddle.reshape(value, [-1, 8 * 10 * 9])\r\n",
    "        value = self.value_fc1(value)\r\n",
    "        value = self.value_act1(value)\r\n",
    "        value = self.value_fc2(value)\r\n",
    "        value = F.tanh(value)\r\n",
    "\r\n",
    "        return policy, value\r\n",
    "\r\n",
    "\r\n",
    "# 策略值网络，用来进行模型的训练\r\n",
    "class PolicyValueNet:\r\n",
    "\r\n",
    "    def __init__(self, model_file=None, use_gpu=True):\r\n",
    "        self.use_gpu = use_gpu\r\n",
    "        self.l2_const = 2e-3    # l2 正则化\r\n",
    "        self.policy_value_net = Net()\r\n",
    "        self.optimizer = paddle.optimizer.Adam(learning_rate=0.001,\r\n",
    "                                               parameters=self.policy_value_net.parameters(),\r\n",
    "                                               weight_decay=self.l2_const)\r\n",
    "        if model_file:\r\n",
    "            net_params = paddle.load(model_file)\r\n",
    "            self.policy_value_net.set_state_dict(net_params)\r\n",
    "\r\n",
    "    # 输入一个批次的状态，输出一个批次的动作概率和状态价值\r\n",
    "    def policy_value(self, state_batch):\r\n",
    "        self.policy_value_net.eval()\r\n",
    "        state_batch = paddle.to_tensor(state_batch)\r\n",
    "        log_act_probs, value = self.policy_value_net(state_batch)\r\n",
    "        act_probs = np.exp(log_act_probs.numpy())\r\n",
    "        return act_probs, value.numpy()\r\n",
    "\r\n",
    "    # 输入棋盘，返回每个合法动作的（动作，概率）元组列表，以及棋盘状态的分数\r\n",
    "    def policy_value_fn(self, board):\r\n",
    "        self.policy_value_net.eval()\r\n",
    "        # 获取合法动作列表\r\n",
    "        legal_positions = board.availables\r\n",
    "        current_state = np.ascontiguousarray(board.current_state().reshape(-1, 9, 10, 9)).astype('float32')\r\n",
    "        current_state = paddle.to_tensor(current_state)\r\n",
    "        # 使用神经网络进行预测\r\n",
    "        log_act_probs, value = self.policy_value_net(current_state)\r\n",
    "        act_probs = np.exp(log_act_probs.numpy().flatten())\r\n",
    "        # 只取出合法动作\r\n",
    "        act_probs = zip(legal_positions, act_probs[legal_positions])\r\n",
    "        # 返回动作概率，以及状态价值\r\n",
    "        return act_probs, value.numpy()\r\n",
    "\r\n",
    "    # 得到模型参数\r\n",
    "    def get_policy_param(self):\r\n",
    "        net_params = self.policy_value_net.state_dict()\r\n",
    "        return net_params\r\n",
    "\r\n",
    "    # 保存模型\r\n",
    "    def save_model(self, model_file):\r\n",
    "        net_params = self.get_policy_param()    # 取得模型参数\r\n",
    "        paddle.save(net_params, model_file)\r\n",
    "\r\n",
    "    # 执行一步训练\r\n",
    "    def train_step(self, state_batch, mcts_probs, winner_batch, lr=0.002):\r\n",
    "        self.policy_value_net.train()\r\n",
    "        # 包装变量\r\n",
    "        state_batch = paddle.to_tensor(state_batch)\r\n",
    "        mcts_probs = paddle.to_tensor(mcts_probs)\r\n",
    "        winner_batch = paddle.to_tensor(winner_batch)\r\n",
    "        # 清零梯度\r\n",
    "        self.optimizer.clear_gradients()\r\n",
    "        # 设置学习率\r\n",
    "        self.optimizer.set_lr(lr)\r\n",
    "        # 前向运算\r\n",
    "        log_act_probs, value = self.policy_value_net(state_batch)\r\n",
    "        value = paddle.reshape(x=value, shape=[-1])\r\n",
    "        # 价值损失\r\n",
    "        value_loss = F.mse_loss(input=value, label=winner_batch)\r\n",
    "        # 策略损失\r\n",
    "        policy_loss = -paddle.mean(paddle.sum(mcts_probs * log_act_probs, axis=1))  # 希望两个向量方向越一致越好\r\n",
    "        # 总的损失，注意l2惩罚已经包含在优化器内部\r\n",
    "        loss = value_loss + policy_loss\r\n",
    "        # 反向传播及优化\r\n",
    "        loss.backward()\r\n",
    "        self.optimizer.minimize(loss)\r\n",
    "        # 计算策略的熵，仅用于评估模型\r\n",
    "        entropy = -paddle.mean(\r\n",
    "            paddle.sum(paddle.exp(log_act_probs) * log_act_probs, axis=1)\r\n",
    "        )\r\n",
    "        return loss.numpy(), entropy.numpy()[0]\r\n",
    "\r\n",
    "\r\n",
    "if __name__ == '__main__':\r\n",
    "    net = Net()\r\n",
    "    test_data = paddle.ones([8, 9, 10, 9])\r\n",
    "    x_act, x_val = net(test_data)\r\n",
    "    print(x_act.shape)  # 8, 2086\r\n",
    "    print(x_val.shape)  # 8, 1\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 五、蒙特卡洛树搜索\n",
    "\n",
    "* 这一节，我们来实现非常关键的蒙特卡洛树搜索。在开始之前，我们先考量一下强化学习价值迭代过程，如下视频所示。这是一个走迷宫游戏s8位置价值为1，价值越高，颜色越红。随着我们智能体的一次次价值迭代，我们可以发现越靠近s8的方块价值越早上升。因为s7走一步就到s8，这样s7就慢慢学到了具有高价值，然后是s4走一步到s7，s4的价值也就慢慢上升。也就是说，价值迭代是一个反向传播的过程。这和我们接下来蒙特卡洛树节点的更新非常相关，如果没有游戏终盘的胜：1，负：-1，我们也是不可能训练的了模型。包括走子概率的学习，也要依赖于状态价值的评估。\n",
    "\n",
    "* 所以，在训练的早期，神经网络的预测值是无效的，只有当搜索树搜索到胜负的时候，才能利用胜负的价值起到一个训练指导作用。在训练的后期，神经网络就会慢慢的从游戏终局往游戏开始学习价值。从而使得不需要搜索到游戏终局，也能用神经网络的预测值作为替代。\n",
    "\n",
    "<div align=center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/9db2eba30f56460ba3d47589f290c86f9f42792ff38640018f1159babda46718\"></div>\n",
    "\n",
    "为了更方便的理解博弈树，如下是甲乙二人的一个博弈示例。\n",
    "\n",
    "<div align=center><img src=\"https://ai-studio-static-online.cdn.bcebos.com/bde6225c59c0442cae2e79f8271348af252bf399d3a946f8837750c1efd3f394\"></div>\n",
    "\n",
    "\n",
    "* 接下来我们正式进入蒙特卡洛树搜索，为了方便理解，我们以井字棋为例，使用puct算法进行4次模拟，每次模拟都需要走到叶节点才停止（也就是尚未扩展的节点）。其中Q值是随意设的，并不代表真实情况。\n",
    "\n",
    "如下，我们进行第一次模拟，根据puct算法得出三个动作值都是0，我们从中选一个最大值，假设就是第一个，得到神经网络估计的Q值0.6，然后我们进行一次反向传播更新其所有父节点的各个参数值（Q，访问次数）等。\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/58242c15151c41fd8dce8415bf32e97c687f388099e44c198f25edc1866ea637)\n",
    "\n",
    "然后我们进行第二次模拟，根据上一次模拟更新的参数再次计算动作值，第一个动作值最大为1.85，我们就选第一个节点。注意：因为第一个节点已经不是叶节点了，所以我们要再进行下一个子节点的选择，同样初次访问，所有动作值是0，假设我们选了第一个子节点，返回0.3，然后对父节点更新时要将Q值添加负号。算出第一个节点的平均Q值为0.15。\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/f993bfa6b1674c8fa885935b6a0c0ec715a929f333e142cfa8dced7770f2b354)\n",
    "\n",
    "同样，我们再进行第三次模拟。\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/04234035fb8a4e378bd6d3aa9b046932368da7888fc440f98aa6e31e301c5f72)\n",
    "\n",
    "第四次模拟。\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/6d7c9720a520437eb632d096992ea393204b7e0544c44a2eb29e1f27ed3c2048)\n",
    "\n",
    "如下所示，蒙特卡洛树搜索一直循环（选节点、扩展和评估、反向更新树中各节点）直到指定的模拟次数，最后进行一步落子。请务必要注意到神经网络在此过程中的缩减搜索树宽度和深度的含义。\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/5d63fd157329483cbe1fcb5542beb0baff33e8462b1a40afa276f399670ddde9)\n",
    "\n",
    "* 随着我们蒙特卡洛树搜索模拟的次数增多，对子节点的估值就会越准确。在本项目的搜索树中，展开搜索树2000个节点之后，以节点的选择次数作为走子概率，然后加上迪利克雷噪声进行真实走子。真实走子之后，根节点变换到走子后的盘面。树搜索次数越多让走子的结果更可靠，平均值依概率收敛到期望。\n",
    "\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/011d564912f04da0934e72790d7e781775fe697ee98941fba376ec8cbf9f7527)\n",
    "\n",
    "加上迪利克雷噪声用于探索未知盘面。\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"蒙特卡洛树搜索\"\"\"\r\n",
    "\r\n",
    "\r\n",
    "import numpy as np\r\n",
    "import copy\r\n",
    "from config import CONFIG\r\n",
    "\r\n",
    "\r\n",
    "def softmax(x):\r\n",
    "    probs = np.exp(x - np.max(x))\r\n",
    "    probs /= np.sum(probs)\r\n",
    "    return probs\r\n",
    "\r\n",
    "\r\n",
    "# 定义叶子节点\r\n",
    "class TreeNode(object):\r\n",
    "    \"\"\"\r\n",
    "    mcts树中的节点，树的子节点字典中，键为动作，值为TreeNode。记录当前节点选择的动作，以及选择该动作后会跳转到的下一个子节点。\r\n",
    "    每个节点跟踪其自身的Q，先验概率P及其访问次数调整的u\r\n",
    "    \"\"\"\r\n",
    "\r\n",
    "    def __init__(self, parent, prior_p):\r\n",
    "        \"\"\"\r\n",
    "        :param parent: 当前节点的父节点\r\n",
    "        :param prior_p:  当前节点被选择的先验概率\r\n",
    "        \"\"\"\r\n",
    "        self._parent = parent\r\n",
    "        self._children = {} # 从动作到TreeNode的映射\r\n",
    "        self._n_visits = 0  # 当前当前节点的访问次数\r\n",
    "        self._Q = 0         # 当前节点对应动作的平均动作价值\r\n",
    "        self._u = 0         # 当前节点的置信上限         # PUCT算法\r\n",
    "        self._P = prior_p\r\n",
    "\r\n",
    "    def expand(self, action_priors):    # 这里把不合法的动作概率全部设置为0\r\n",
    "        \"\"\"通过创建新子节点来展开树\"\"\"\r\n",
    "        for action, prob in action_priors:\r\n",
    "            if action not in self._children:\r\n",
    "                self._children[action] =  TreeNode(self, prob)\r\n",
    "\r\n",
    "    def select(self, c_puct):\r\n",
    "        \"\"\"\r\n",
    "        在子节点中选择能够提供最大的Q+U的节点\r\n",
    "        return: (action, next_node)的二元组\r\n",
    "        \"\"\"\r\n",
    "        return max(self._children.items(),\r\n",
    "                   key=lambda act_node: act_node[1].get_value(c_puct))\r\n",
    "\r\n",
    "    def get_value(self, c_puct):\r\n",
    "        \"\"\"\r\n",
    "        计算并返回此节点的值，它是节点评估Q和此节点的先验的组合\r\n",
    "        c_puct: 控制相对影响（0， inf）\r\n",
    "        \"\"\"\r\n",
    "        self._u = (c_puct * self._P *\r\n",
    "                   np.sqrt(self._parent._n_visits) / (1 + self._n_visits))\r\n",
    "        return self._Q + self._u\r\n",
    "\r\n",
    "    def update(self, leaf_value):\r\n",
    "        \"\"\"\r\n",
    "        从叶节点评估中更新节点值\r\n",
    "        leaf_value: 这个子节点的评估值来自当前玩家的视角\r\n",
    "        \"\"\"\r\n",
    "        # 统计访问次数\r\n",
    "        self._n_visits += 1\r\n",
    "        # 更新Q值，取决于所有访问次数的平均树，使用增量式更新方式\r\n",
    "        self._Q += 1.0 * (leaf_value - self._Q) / self._n_visits\r\n",
    "\r\n",
    "    # 使用递归的方法对所有节点（当前节点对应的支线）进行一次更新\r\n",
    "    def update_recursive(self, leaf_value):\r\n",
    "        \"\"\"就像调用update()一样，但是对所有直系节点进行更新\"\"\"\r\n",
    "        # 如果它不是根节点，则应首先更新此节点的父节点\r\n",
    "        if self._parent:\r\n",
    "            self._parent.update_recursive(-leaf_value)\r\n",
    "        self.update(leaf_value)\r\n",
    "\r\n",
    "    def is_leaf(self):\r\n",
    "        \"\"\"检查是否是叶节点，即没有被扩展的节点\"\"\"\r\n",
    "        return self._children == {}\r\n",
    "\r\n",
    "    def is_root(self):\r\n",
    "        return self._parent is None\r\n",
    "\r\n",
    "\r\n",
    "# 蒙特卡洛搜索树\r\n",
    "class MCTS(object):\r\n",
    "\r\n",
    "    def __init__(self, policy_value_fn, c_puct=5, n_playout=2000):\r\n",
    "        \"\"\"policy_value_fn: 接收board的盘面状态，返回落子概率和盘面评估得分\"\"\"\r\n",
    "        self._root = TreeNode(None, 1.0)\r\n",
    "        self._policy = policy_value_fn\r\n",
    "        self._c_puct = c_puct\r\n",
    "        self._n_playout = n_playout\r\n",
    "\r\n",
    "    def _playout(self, state):\r\n",
    "        \"\"\"\r\n",
    "        进行一次搜索，根据叶节点的评估值进行反向更新树节点的参数\r\n",
    "        注意：state已就地修改，因此必须提供副本\r\n",
    "        \"\"\"\r\n",
    "        node = self._root\r\n",
    "        while True:\r\n",
    "            if node.is_leaf():\r\n",
    "                break\r\n",
    "            # 贪心算法选择下一步行动\r\n",
    "            action, node = node.select(self._c_puct)\r\n",
    "            state.do_move(action)\r\n",
    "\r\n",
    "        # 使用网络评估叶子节点，网络输出（动作，概率）元组p的列表以及当前玩家视角的得分[-1, 1]\r\n",
    "        action_probs, leaf_value = self._policy(state)\r\n",
    "        # 查看游戏是否结束\r\n",
    "        end, winner = state.game_end()\r\n",
    "        if not end:\r\n",
    "            node.expand(action_probs)\r\n",
    "        else:\r\n",
    "            # 对于结束状态，将叶子节点的值换成1或-1\r\n",
    "            if winner == -1:    # Tie\r\n",
    "                leaf_value = 0.0\r\n",
    "            else:\r\n",
    "                leaf_value = (\r\n",
    "                    1.0 if winner == state.get_current_player_id() else -1.0\r\n",
    "                )\r\n",
    "        # 在本次遍历中更新节点的值和访问次数\r\n",
    "        # 必须添加符号，因为两个玩家共用一个搜索树\r\n",
    "        node.update_recursive(-leaf_value)\r\n",
    "\r\n",
    "    def get_move_probs(self, state, temp=1e-3):\r\n",
    "        \"\"\"\r\n",
    "        按顺序运行所有搜索并返回可用的动作及其相应的概率\r\n",
    "        state:当前游戏的状态\r\n",
    "        temp:介于（0， 1]之间的温度参数\r\n",
    "        \"\"\"\r\n",
    "        for n in range(self._n_playout):\r\n",
    "            state_copy = copy.deepcopy(state)\r\n",
    "            self._playout(state_copy)\r\n",
    "\r\n",
    "        # 跟据根节点处的访问计数来计算移动概率\r\n",
    "        act_visits= [(act, node._n_visits)\r\n",
    "                     for act, node in self._root._children.items()]\r\n",
    "        acts, visits = zip(*act_visits)\r\n",
    "        act_probs = softmax(1.0 / temp * np.log(np.array(visits) + 1e-10))\r\n",
    "        return acts, act_probs\r\n",
    "\r\n",
    "    def update_with_move(self, last_move):\r\n",
    "        \"\"\"\r\n",
    "        在当前的树上向前一步，保持我们已经直到的关于子树的一切\r\n",
    "        \"\"\"\r\n",
    "        if last_move in self._root._children:\r\n",
    "            self._root = self._root._children[last_move]\r\n",
    "            self._root._parent = None\r\n",
    "        else:\r\n",
    "            self._root = TreeNode(None, 1.0)\r\n",
    "\r\n",
    "    def __str__(self):\r\n",
    "        return 'MCTS'\r\n",
    "\r\n",
    "\r\n",
    "# 基于MCTS的AI玩家\r\n",
    "class MCTSPlayer(object):\r\n",
    "\r\n",
    "    def __init__(self, policy_value_function, c_puct=5, n_playout=2000, is_selfplay=0):\r\n",
    "        self.mcts = MCTS(policy_value_function, c_puct, n_playout)\r\n",
    "        self._is_selfplay = is_selfplay\r\n",
    "        self.agent = \"AI\"\r\n",
    "\r\n",
    "    def set_player_ind(self, p):\r\n",
    "        self.player = p\r\n",
    "\r\n",
    "    # 重置搜索树\r\n",
    "    def reset_player(self):\r\n",
    "        self.mcts.update_with_move(-1)\r\n",
    "\r\n",
    "    def __str__(self):\r\n",
    "        return 'MCTS {}'.format(self.player)\r\n",
    "\r\n",
    "    # 得到行动\r\n",
    "    def get_action(self, board, temp=1e-3, return_prob=0):\r\n",
    "        # 像alphaGo_Zero论文一样使用MCTS算法返回的pi向量\r\n",
    "        move_probs = np.zeros(2086)\r\n",
    "\r\n",
    "        acts, probs = self.mcts.get_move_probs(board, temp)\r\n",
    "        move_probs[list(acts)] = probs\r\n",
    "        if self._is_selfplay:\r\n",
    "            # 添加Dirichlet Noise进行探索（自我对弈需要）\r\n",
    "            move = np.random.choice(\r\n",
    "                acts,\r\n",
    "                p=0.75*probs + 0.25*np.random.dirichlet(CONFIG['dirichlet'] * np.ones(len(probs)))\r\n",
    "            )\r\n",
    "            # 更新根节点并重用搜索树\r\n",
    "            self.mcts.update_with_move(move)\r\n",
    "        else:\r\n",
    "            # 使用默认的temp=1e-3，它几乎相当于选择具有最高概率的移动\r\n",
    "            move = np.random.choice(acts, p=probs)\r\n",
    "            # 重置根节点\r\n",
    "            self.mcts.update_with_move(-1)\r\n",
    "        if return_prob:\r\n",
    "            return move, move_probs\r\n",
    "        else:\r\n",
    "            return move"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 六、影分身训练\n",
    "\n",
    "* 本项目的同步并行是基于数据库的方式来实现的。每个分身自我对弈结束都把自己的数据添加到同一个data_buffer中去。每次自我对弈结束，都拿到主体更新的最新模型进行下一次的对弈。主体每隔固定时间读取data_buffer进行一次模型训练。\n",
    "![](https://ai-studio-static-online.cdn.bcebos.com/1c576a089d694fa89617bdfe7109f7da2c11bdeffbab4bc799124d3e98368cba)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"自我对弈收集数据\"\"\"\r\n",
    "\r\n",
    "\r\n",
    "from collections import deque\r\n",
    "import copy\r\n",
    "import os\r\n",
    "import pickle\r\n",
    "import time\r\n",
    "from game import Board, Game, move_action2move_id, move_id2move_action, flip_map\r\n",
    "from net import PolicyValueNet\r\n",
    "from mcts import MCTSPlayer\r\n",
    "from config import CONFIG\r\n",
    "\r\n",
    "\r\n",
    "# 定义整个对弈收集数据流程\r\n",
    "class CollectPipeline:\r\n",
    "\r\n",
    "    def __init__(self, init_model=None):\r\n",
    "        # 象棋逻辑和棋盘\r\n",
    "        self.board = Board()\r\n",
    "        self.game = Game(self.board)\r\n",
    "        # 对弈参数\r\n",
    "        self.temp = 1   # 温度\r\n",
    "        self.n_playout = CONFIG['play_out']  # 每次移动的模拟次数\r\n",
    "        self.c_puct = CONFIG['c_puct']       # u的权重\r\n",
    "        self.buffer_size = CONFIG['buffer_size']    # 经验池大小\r\n",
    "        self.data_buffer = deque(maxlen=self.buffer_size)\r\n",
    "        self.iters = 0\r\n",
    "\r\n",
    "    # 从主体加载模型\r\n",
    "    def load_model(self, model_path=CONFIG['model_path']):\r\n",
    "        try:\r\n",
    "            self.policy_value_net = PolicyValueNet(model_file=model_path)\r\n",
    "            print('已加载最新模型')\r\n",
    "        except:\r\n",
    "            self.policy_value_net = PolicyValueNet()\r\n",
    "            print('已加载初始模型')\r\n",
    "        self.mcts_player = MCTSPlayer(self.policy_value_net.policy_value_fn,\r\n",
    "                                      c_puct=self.c_puct,\r\n",
    "                                      n_playout=self.n_playout,\r\n",
    "                                      is_selfplay=1)\r\n",
    "\r\n",
    "    def get_equi_data(self, play_data):\r\n",
    "        \"\"\"左右对称变换，扩充数据集一倍，加速一倍训练速度\"\"\"\r\n",
    "        extend_data = []\r\n",
    "        # 棋盘状态shape is [9, 10, 9], 走子概率，赢家\r\n",
    "        for state, mcts_prob, winner in play_data:\r\n",
    "            # 原始数据\r\n",
    "            extend_data.append((state, mcts_prob, winner))\r\n",
    "            # 水平翻转后的数据\r\n",
    "            state_flip = state.transpose([1, 2, 0])\r\n",
    "            state = state.transpose([1, 2, 0])\r\n",
    "            for i in range(10):\r\n",
    "                for j in range(9):\r\n",
    "                    state_flip[i][j] = state[i][8-j]\r\n",
    "            state_flip = state_flip.transpose([2, 0, 1])\r\n",
    "            mcts_prob_flip = copy.deepcopy(mcts_prob)\r\n",
    "            for i in range(len(mcts_prob_flip)):\r\n",
    "                mcts_prob_flip[i] = mcts_prob[move_action2move_id[flip_map(move_id2move_action[i])]]\r\n",
    "            extend_data.append((state_flip, mcts_prob_flip, winner))\r\n",
    "        return extend_data\r\n",
    "\r\n",
    "    def collect_selfplay_data(self, n_games=1):\r\n",
    "        # 收集自我对弈的数据\r\n",
    "        for i in range(n_games):\r\n",
    "            self.load_model()   # 从本体处加载最新模型\r\n",
    "            winner, play_data = self.game.start_self_play(self.mcts_player, temp=self.temp, is_shown=False)\r\n",
    "            play_data = list(play_data)[:]\r\n",
    "            self.episode_len = len(play_data)\r\n",
    "            # 增加数据\r\n",
    "            play_data = self.get_equi_data(play_data)\r\n",
    "\r\n",
    "            if os.path.exists(CONFIG['train_data_buffer_path']):\r\n",
    "                while True:\r\n",
    "                    try:\r\n",
    "                        with open(CONFIG['train_data_buffer_path'], 'rb') as data_dict:\r\n",
    "                            data_file = pickle.load(data_dict)\r\n",
    "                            self.data_buffer = data_file['data_buffer']\r\n",
    "                            self.iters = data_file['iters']\r\n",
    "                            del data_file\r\n",
    "                            self.iters += 1\r\n",
    "                            self.data_buffer.extend(play_data)\r\n",
    "                        print('成功载入数据')\r\n",
    "                        break\r\n",
    "                    except:\r\n",
    "                        time.sleep(30)\r\n",
    "            else:\r\n",
    "                self.data_buffer.extend(play_data)\r\n",
    "                self.iters += 1\r\n",
    "            data_dict = {'data_buffer': self.data_buffer, 'iters': self.iters}\r\n",
    "            with open(CONFIG['train_data_buffer_path'], 'wb') as data_file:\r\n",
    "                pickle.dump(data_dict, data_file)\r\n",
    "        return self.iters\r\n",
    "\r\n",
    "    def run(self):\r\n",
    "        \"\"\"开始收集数据\"\"\"\r\n",
    "        try:\r\n",
    "            while True:\r\n",
    "                iters = self.collect_selfplay_data()\r\n",
    "                print('batch i: {}, episode_len: {}'.format(\r\n",
    "                    iters, self.episode_len))\r\n",
    "        except KeyboardInterrupt:\r\n",
    "            print('\\n\\rquit')\r\n",
    "\r\n",
    "\r\n",
    "collecting_pipeline = CollectPipeline(init_model='current_policy.model')\r\n",
    "collecting_pipeline.run()\r\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"使用收集到数据进行训练\"\"\"\r\n",
    "\r\n",
    "\r\n",
    "import random\r\n",
    "import numpy as np\r\n",
    "import pickle\r\n",
    "import time\r\n",
    "from net import PolicyValueNet\r\n",
    "from config import CONFIG\r\n",
    "\r\n",
    "\r\n",
    "# 定义整个训练流程\r\n",
    "class TrainPipeline:\r\n",
    "\r\n",
    "    def __init__(self, init_model=None):\r\n",
    "        # 训练参数\r\n",
    "        self.learn_rate = 1e-3\r\n",
    "        self.lr_multiplier = 1  # 基于KL自适应的调整学习率\r\n",
    "        self.temp = 1.0\r\n",
    "        self.batch_size = CONFIG['batch_size']  # 训练的batch大小\r\n",
    "        self.epochs = CONFIG['epochs']  # 每次更新的train_step数量\r\n",
    "        self.kl_targ = CONFIG['kl_targ']  # kl散度控制\r\n",
    "        self.check_freq = 100  # 保存模型的频率\r\n",
    "        self.game_batch_num = CONFIG['game_batch_num']  # 训练更新的次数\r\n",
    "\r\n",
    "        if init_model:\r\n",
    "            try:\r\n",
    "                self.policy_value_net = PolicyValueNet(model_file=init_model)\r\n",
    "                print('已加载上次最终模型')\r\n",
    "            except:\r\n",
    "                # 从零开始训练\r\n",
    "                print('模型路径不存在，从零开始训练')\r\n",
    "                self.policy_value_net = PolicyValueNet()\r\n",
    "        else:\r\n",
    "            print('从零开始训练')\r\n",
    "            self.policy_value_net = PolicyValueNet()\r\n",
    "\r\n",
    "    def policy_updata(self):\r\n",
    "        \"\"\"更新策略价值网络\"\"\"\r\n",
    "        mini_batch = random.sample(self.data_buffer, self.batch_size)\r\n",
    "\r\n",
    "        state_batch = [data[0] for data in mini_batch]\r\n",
    "        state_batch = np.array(state_batch).astype('float32')\r\n",
    "\r\n",
    "        mcts_probs_batch = [data[1] for data in mini_batch]\r\n",
    "        mcts_probs_batch = np.array(mcts_probs_batch).astype('float32')\r\n",
    "\r\n",
    "        winner_batch = [data[2] for data in mini_batch]\r\n",
    "        winner_batch = np.array(winner_batch).astype('float32')\r\n",
    "\r\n",
    "        # 旧的策略，旧的价值函数\r\n",
    "        old_probs, old_v = self.policy_value_net.policy_value(state_batch)\r\n",
    "\r\n",
    "        for i in range(self.epochs):\r\n",
    "            loss, entropy = self.policy_value_net.train_step(\r\n",
    "                state_batch,\r\n",
    "                mcts_probs_batch,\r\n",
    "                winner_batch,\r\n",
    "                self.learn_rate * self.lr_multiplier\r\n",
    "            )\r\n",
    "            # 新的策略，新的价值函数\r\n",
    "            new_probs, new_v = self.policy_value_net.policy_value(state_batch)\r\n",
    "\r\n",
    "            kl = np.mean(np.sum(old_probs * (\r\n",
    "                np.log(old_probs + 1e-10) - np.log(new_probs + 1e-10)),\r\n",
    "                                axis=1))\r\n",
    "            if kl > self.kl_targ * 4:  # 如果KL散度很差，则提前终止\r\n",
    "                break\r\n",
    "\r\n",
    "        # 自适应调整学习率\r\n",
    "        if kl > self.kl_targ * 2 and self.lr_multiplier > 0.1:\r\n",
    "            self.lr_multiplier /= 1.5\r\n",
    "        elif kl < self.kl_targ / 2 and self.lr_multiplier < 10:\r\n",
    "            self.lr_multiplier *= 1.5\r\n",
    "\r\n",
    "        explained_var_old = (1 -\r\n",
    "                             np.var(np.array(winner_batch) - old_v.flatten()) /\r\n",
    "                             np.var(np.array(winner_batch)))\r\n",
    "        explained_var_new = (1 -\r\n",
    "                             np.var(np.array(winner_batch) - new_v.flatten()) /\r\n",
    "                             np.var(np.array(winner_batch)))\r\n",
    "\r\n",
    "        print((\"kl:{:.5f},\"\r\n",
    "               \"lr_multiplier:{:.3f},\"\r\n",
    "               \"loss:{},\"\r\n",
    "               \"entropy:{},\"\r\n",
    "               \"explained_var_old:{:.3f},\"\r\n",
    "               \"explained_var_new:{:.3f}\"\r\n",
    "               ).format(kl,\r\n",
    "                        self.lr_multiplier,\r\n",
    "                        loss,\r\n",
    "                        entropy,\r\n",
    "                        explained_var_old,\r\n",
    "                        explained_var_new))\r\n",
    "        return loss, entropy\r\n",
    "\r\n",
    "    def run(self):\r\n",
    "        \"\"\"开始训练\"\"\"\r\n",
    "        try:\r\n",
    "            for i in range(self.game_batch_num):\r\n",
    "                time.sleep(30)  # 每10分钟更新一次模型\r\n",
    "                while True:\r\n",
    "                    try:\r\n",
    "                        with open(CONFIG['train_data_buffer_path'], 'rb') as data_dict:\r\n",
    "                            data_file = pickle.load(data_dict)\r\n",
    "                            self.data_buffer = data_file['data_buffer']\r\n",
    "                            self.iters = data_file['iters']\r\n",
    "                            del data_file\r\n",
    "                        print('已载入数据')\r\n",
    "                        break\r\n",
    "                    except:\r\n",
    "                        time.sleep(30)\r\n",
    "                print('step i {}: '.format(self.iters))\r\n",
    "                if len(self.data_buffer) > self.batch_size:\r\n",
    "                    loss, entropy = self.policy_updata()\r\n",
    "                # 保存模型\r\n",
    "                self.policy_value_net.save_model(CONFIG['model_path'])\r\n",
    "                if (i + 1) % self.check_freq == 0:\r\n",
    "                    print('current selfplay batch: {}'.format(i + 1))\r\n",
    "                    self.policy_value_net.save_model('models/current_policy_batch{}.model'.format(i + 1))\r\n",
    "        except KeyboardInterrupt:\r\n",
    "            print('\\n\\rquit')\r\n",
    "\r\n",
    "\r\n",
    "training_pipeline = TrainPipeline(init_model='current_policy.model')\r\n",
    "training_pipeline.run()\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 七、参考与致谢\n",
    "\n",
    "* 本项目主要参考的资料如下，十分感谢大佬们的分享\n",
    "\n",
    "1、程世东 https://zhuanlan.zhihu.com/p/34433581 （中国象棋cchesszero ）\n",
    "\n",
    "2、AI在打野 https://aistudio.baidu.com/aistudio/projectdetail/1403398 （用paddle打造的五子棋AI）\n",
    "\n",
    "3、junxiaosong  https://github.com/junxiaosong/AlphaZero_Gomoku (五子棋alphazero)\n",
    "\n",
    "4、书籍：边做边学深度强化学习：PyTorch 程序设计实践\n",
    "\n",
    "5、书籍：强化学习第二版\n",
    "\n",
    "后续应该会对该AI继续训练下去，亲手造一个超强的下棋AI简直太酷了！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
